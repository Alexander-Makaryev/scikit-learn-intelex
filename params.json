{
  "name": "DAAL4PY",
  "tagline": "A convenient Python API to Intel(R) Data Analytics Acceleration Library (Intel(R) DAAL)",
  "body": "With this API your Python programs can use Intel(R) DAAL algorithms in just one line:\r\n```\r\nkmeans_init(data, 10, t_method=\"plusPlusDense\")\r\n```\r\nYou can even run this on a cluster by simple adding a keyword-parameter\r\n```\r\nkmeans_init(data, 10, t_method=\"plusPlusDense\", distributed=TRUE)\r\n```\r\n\r\n**_This is a technical preview, not a product. Intel might decide to discontinue this project at any time._**\r\n\r\n## OVERVIEW\r\nAll algorithms work the same way:\r\n1. Instantiate and parametrize\r\n2. Run/compute on input data\r\n\r\nThe below tables list the accepted arguments. Those with no default (None) are required arguments. All other arguments with defaults are optional and can be provided as keyword arguments (like optarg=77).\r\nEach algorithm returns a class-like object with properties as its result.\r\n\r\nFor algorithms with training and prediction, simply extract the 'model' property from the result returned by the training and pass it in as the (second) input argument. \r\n\r\nNote that all input objects and the result/model properties are native types, e.g. standard types (integer, float, numpy arrays, ...). Additionally, if you provide the name of a csv-file as an input argument daal4py will work on the entire file content.\r\n\r\n## MULTIPROCESSING/DISTRIBUTED PROCESSING\r\nSome algorithms can work on several processes (distributed). Currently the following algorithms support distribution:\r\n•\tpca\r\n•\tsvd\r\n•\tlinear_regression_training\r\n•\tmultinomial_naive_bayes_training\r\n•\tkmeans and kmeans_init\r\n\r\nThe provided binaries use the Intel® MPI library. \r\n\r\nNote: Distribution needs to be initialized with a call to daalinit() and before exiting the program a call to daalfini() is required.\r\n\r\n### SPV: Single Process View\r\nThe Single-Process-View (SVP) mode lets you program as if you had a single process. Distribution is done under the hood. This mode works like as follows:\r\n1. Your program begins with ```daalinit()```\r\n2. Provide several arrays or csv-files (like one per process). This represents the partitioning of your data.\r\n3. When calling an algorithm add the argument ```distributed=True```.\r\n\r\n### SPMD: Single Program Multiple Data\r\nYou can also program in SPMD-style, e.g. like the usual MPI style. You do not need mpi4py for daal4py/daal4r, but you can use it of course if required. Any of the above algorithms return a the usable result on all processes. Note: daal4py provides ```my_procid()``` and ```num_procs()``` to able to properly work in SPMD mode. This mode works like as follows:\r\n1.\tYour program begins with ```daalinit(spmd=True)```\r\n2.\tProvide the same number of arrays or csv-files per process. This represents the partitioning of your data.\r\n3.\tWhen calling an algorithm add the argument ```distributed=True```.\r\n\r\n### Starting Processes\r\nCurrently we support the MPI used by Intel® Concurrent Collections (Intel CnC), which is usually Intel® MPI Library. Just use mpirun as usual and set the communication-protocol to MPI like this\r\n```\r\nmpirun –genv DIST_CNC=MPI –n <num-procs> python <your-program>\r\n```\r\n\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}